# ğŸš€ NLP Tokenization & Language Modeling from Scratch

This project implements core **Natural Language Processing (NLP)** components from scratch **without using external NLP libraries** like NLTK or Hugging Face.  

## ğŸ“Œ Overview  

This repository covers three major NLP tasks:  

- ğŸ· **Task 1**: Implementing a **WordPiece Tokenizer**.  
- ğŸ§  **Task 2**: Building a **Word2Vec (CBOW) Model** using PyTorch.  
- ğŸ”¥ **Task 3**: Training a **Neural Language Model (MLP-based)** with three architectural variations.  

---

## ğŸ¯ Features  

âœ… **Fully custom implementation** of a WordPiece Tokenizer.  
âœ… **Word2Vec (CBOW) model** using PyTorch.  
âœ… **Neural Language Model** trained for next-word prediction.  
âœ… **PyTorch-based training pipeline** with loss visualization.  
âœ… **Evaluation metrics** including cosine similarity, accuracy, and perplexity.  

---

## ğŸ“œ Task 1: WordPiece Tokenizer  

### ğŸ›  Implementation Details  

- **Preprocessing**: Cleans and processes raw text data.  
- **Vocabulary Construction**: Extracts subword tokens and saves them in `vocabulary_{GroupNo}.txt`.  
- **Tokenization**: Converts sentences into subword tokens.  

### ğŸ“‚ Deliverables  

ğŸ“„ `task1.py` - Contains the **WordPieceTokenizer** class.  
ğŸ“„ `vocabulary_{GroupNo}.txt` - Stores the generated vocabulary.  
ğŸ“„ `tokenized_{GroupNo}.json` - Output JSON file with tokenized sentences.  

ğŸ“¸ **Sample Output:**  
*(Insert Tokenized Output Image Here)*  

---

## ğŸ§  Task 2: Word2Vec Model (CBOW)  

### ğŸ›  Implementation Details  

- **Dataset Preparation**: Implements `Word2VecDataset` to create training data.  
- **Word2Vec Model**: Implements a CBOW-based neural network using PyTorch.  
- **Training Function**: Manages the training pipeline.  
- **Similarity Calculation**: Computes cosine similarity for token triplets.  

### ğŸ“‚ Deliverables  

ğŸ“„ `task2.py` - Contains `Word2VecDataset` and `Word2VecModel` classes.  
ğŸ“¦ **Model checkpoint** after training.  
ğŸ“Š **Loss curve visualization**.  
ğŸ”— **Identified token triplets** based on cosine similarity.  

ğŸ“¸ **Loss Graph Output:**  
*(Insert Graph Image Here)*  

ğŸ“¸ **Token Similarity Example:**  
*(Insert Token Similarity Output Image Here)*  

---

## ğŸ”¥ Task 3: Neural Language Model (MLP)  

### ğŸ›  Implementation Details  

- **Dataset Preparation**: Implements `NeuralLMDataset` for next-word prediction.  
- **Three Neural Network Variations**:  
  - ğŸ— **NeuralLM1**: Baseline model.  
  - ğŸš€ **NeuralLM2**: Modified activation functions and layers.  
  - ğŸ”¬ **NeuralLM3**: Increased input token size.  
- **Training Function**: Handles training across all models.  
- **Evaluation Metrics**: Computes **accuracy and perplexity**.  
- **Next Token Prediction**: Predicts the next three tokens for test sentences.  

### ğŸ“‚ Deliverables  

ğŸ“„ `task3.py` - Contains dataset class and three model architectures.  
ğŸ“Š **Training and validation loss curves**.  
ğŸ“ˆ **Accuracy and perplexity scores**.  
ğŸ“œ **Token predictions** for `test.txt`.  

ğŸ“¸ **Loss Curves for Models:**  
*(Insert Loss Graphs Here)*  

ğŸ“¸ **Accuracy and Perplexity Results:**  
*(Insert Results Here)*  

---

## âš¡ Setup and Execution  

### ğŸ”§ Prerequisites  

Ensure you have the following installed:  

- ğŸ–¥ **Python 3.x**  
- ğŸ“¦ **PyTorch**  
- ğŸ“¦ **NumPy**  
- ğŸ“¦ **Pandas**  

#### ğŸ“¥ Installation
```bash
pip install torch numpy pandas
``` 

<!-- ```markdown -->
### ğŸš€ Running the Scripts  

Run the following commands to execute each task:  

**Task 1: WordPiece Tokenizer**  
```bash
python WordPieceTokeniser.py
```

**Task 2: Word2Vec Training**  
```bash
python Word2Vec_model.py
```

**Task 3: Neural Language Model**  
```bash
python task3.py
```

---

## ğŸ“Š Results and Observations  

âœ… The **WordPiece Tokenizer** effectively segments words into subwords.  
âœ… The **CBOW Word2Vec model** captures meaningful word relationships.  
âœ… The **Neural Language Models** exhibit varying performance based on architecture choices.  
âœ… **Higher token context** improves next-word prediction accuracy.  

---

## ğŸ”® Future Improvements  

ğŸ”¹ **Implement positional encoding** for better embeddings.  
ğŸ”¹ **Experiment with Transformer-based models** for improved performance.  
ğŸ”¹ **Extend vocabulary using larger datasets**.  

---

## ğŸ‘¨â€ğŸ’» Contributors  

ğŸ’¡ **[Your Name]** - Implementation & Documentation  
ğŸ’¡ **[Your Team (if applicable)]**  

---

## ğŸ“œ License  

ğŸ“ This project is licensed under the **MIT License**. See `LICENSE` for details.  

---


